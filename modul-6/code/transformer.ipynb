{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Import Library**\n",
    "\n",
    "Sebelum membuat Transformer from scratch, import library terlebih dahulu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Word Embedding**\n",
    "\n",
    "Seperti task NLP lainnya, kita memetakan token input ke dalam sebuah matrix. Dalam hal ini kita memerlukan Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 512])\n",
      "tensor([[[ 18.0635,  28.5057, -25.3359,  ...,  28.3018,  18.6842, -40.6766],\n",
      "         [  5.7509,  14.3622,  -6.0185,  ...,  27.5548,  13.9551,   3.0788],\n",
      "         [-37.3428, -22.3181,   1.7631,  ...,  18.8375,  -7.4120, -36.4528],\n",
      "         [-32.6975, -26.0815, -18.5710,  ...,  56.4819, -29.7464, -15.3596],\n",
      "         [-23.8286, -20.0306,  31.4403,  ..., -31.8298,  39.9772,  29.4495]],\n",
      "\n",
      "        [[ 13.7110,  16.9010,  -0.5562,  ...,  -2.8209,  10.2214,  -1.1217],\n",
      "         [-16.0309, -35.5362,   9.4748,  ...,  27.0622,   0.7215, -25.1355],\n",
      "         [  9.8960,  35.8026, -32.6231,  ..., -10.0365,  36.1730,  -5.7838],\n",
      "         [ 28.1151, -17.5125,  -5.2629,  ..., -18.0260,   2.9462,  25.7082],\n",
      "         [  2.6106, -29.2682,  11.5294,  ...,  -7.8573, -15.2650,   4.5868]]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class WordEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, padding_idx=0):\n",
    "        super(WordEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=padding_idx)\n",
    "        self.d_model = d_model\n",
    "        self.scale = math.sqrt(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        return x * self.scale\n",
    "\n",
    "token_ids = torch.tensor([\n",
    "    [1, 2, 3, 4, 5],    \n",
    "    [6, 7, 8, 9, 10]      \n",
    "])  # shape: [batch_size=2, seq_len=5]  \n",
    "\n",
    "vocab_size = 10000\n",
    "d_model = 512\n",
    "\n",
    "embedding_layer = WordEmbedding(vocab_size, d_model)\n",
    "word_embedding_output = embedding_layer(token_ids)\n",
    "print(word_embedding_output.shape) \n",
    "print(word_embedding_output)  # Output the word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Positional Encoding**\n",
    "\n",
    "Karena Transformer tidak mengandalkan urutan alami dari data (seperti urutan waktu pada RNN), informasi posisi token ditambahkan secara eksplisit melalui positional encoding. Vektor ini menggunakan fungsi sinus dan cosinus untuk memberikan “koordinat” yang unik pada tiap token sehingga model dapat memahami urutan kata dalam kalimat. \n",
    "\n",
    "Setiap token dalam input ditambahkan dengan vektor posisi yang dihitung menggunakan fungsi sinus dan cosinus. Vektor ini memiliki dimensi yang sama dengan embedding sehingga penjumlahan antara embedding dan positional encoding dapat dilakukan secara langsung.\n",
    "\n",
    "Positional encoding memberikan representasi unik berbasis pola periodik, mirip dengan bagaimana gelombang sinusoidal dapat merepresentasikan informasi dalam sinyal, sehinnga bisa dianggap seperti memberikan `koordinat` untuk setiap token. Kenapa fungsi sinus dan cosinus? Karena Penggunaan fungsi sinus dan cosinus memastikan bahwa posisi relatif antara kata-kata tetap terjaga, bahkan ketika panjang urutan bervariasi. Hal ini penting karena Transformer tidak memiliki mekanisme memori seperti RNN.\n",
    "\n",
    "Rumus dari Postional Encoding adalah sebagai berikut:\n",
    "$$\n",
    "PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{\\frac{2i}{d}}}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{\\frac{2i}{d}}}\\right)\n",
    "$$\n",
    "\n",
    "- `d` adalah dimensi embedding\n",
    "- `pos` adalah index dari posisi\n",
    "- `i` adalah index dari dimensi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5000, 512])\n",
      "torch.Size([5000, 1])\n",
      "Shape input setelah positional encoding: torch.Size([2, 5, 512])\n",
      "Output: tensor([[[ 18.0635,  29.5057, -25.3359,  ...,  29.3018,  18.6842, -39.6766],\n",
      "         [  6.5924,  14.9025,  -5.1967,  ...,  28.5548,  13.9552,   4.0788],\n",
      "         [-36.4335, -22.7343,   2.6995,  ...,  19.8375,  -7.4118, -35.4528],\n",
      "         [-32.5564, -27.0715, -18.3259,  ...,  57.4819, -29.7461, -14.3596],\n",
      "         [-24.5854, -20.6842,  30.7832,  ..., -30.8298,  39.9776,  30.4495]],\n",
      "\n",
      "        [[ 13.7110,  17.9010,  -0.5562,  ...,  -1.8209,  10.2214,  -0.1217],\n",
      "         [-15.1895, -34.9959,  10.2967,  ...,  28.0622,   0.7216, -24.1355],\n",
      "         [ 10.8053,  35.3865, -31.6867,  ...,  -9.0365,  36.1732,  -4.7838],\n",
      "         [ 28.2562, -18.5025,  -5.0178,  ..., -17.0260,   2.9465,  26.7082],\n",
      "         [  1.8538, -29.9218,  10.8723,  ...,  -6.8573, -15.2646,   5.5868]]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class PositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # Buat tensor posisi dengan ukuran (max_len, d_model)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        print(pe.shape)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        print(position.shape)\n",
    "        # Hitung pembagi frekuensi\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        # Aplikasi rumus sinus dan cosinus\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        # Tambahkan dimensi batch agar dapat langsung ditambahkan ke embedding\n",
    "        pe = pe.unsqueeze(0)  # Shape: [1, max_len, d_model]\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, d_model]\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return x\n",
    "    \n",
    "# Contoh penggunaan:\n",
    "d_model = 512\n",
    "pe_layer = PositionalEncoding(d_model)\n",
    "pe_output = pe_layer(word_embedding_output)\n",
    "print(\"Shape input setelah positional encoding:\", pe_output.shape)\n",
    "print(\"Output:\", pe_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Attention Mechanism**\n",
    "\n",
    "Bagaimana cara transformer mengerti konteks dari suatu sequence - kata per kata?. Contohnya pada kalimat \"Sigra membeli sate dan rasanya enak sekali\". Kata \"enak\" bisa saja merujuk ke \"sate\" ataupun \"Sigra\". Attention Mechanism memungkinkan model untuk mengerti hubungan kata per kata dengan cara melihat bagaimana mirip setiap kata yang ada di dalam sequence. Kata \"sate\" dan \"enak\" akan memiliki kemiripan lebih tinggi dibanding kata yang lain, hal ini akan mengakibatkan bagaimana kata \"enak\" akan di encode di dalam transformer.\n",
    "\n",
    "#### **Single Head Attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape output SelfAttention: torch.Size([2, 5, 512])\n",
      "Shape attention weights: torch.Size([2, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.out_linear = nn.Linear(d_model, d_model)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        # self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        # Q, K, V: [batch_size, seq_len, d_model]\n",
    "\n",
    "        # Hitung dot product attention\n",
    "        dp_attention = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_model)\n",
    "        # dp_attention: [batch_size, seq_len, seq_len]\n",
    "    \n",
    "        if mask is not None:\n",
    "             # Mask: [batch_size, 1, seq_len, seq_len] or [1, 1, seq_len, seq_len] or [seq_len, seq_len] (broadcastable)\n",
    "            dp_attention = dp_attention.masked_fill(mask == 0, -1e9) # Asumsi mask adalah 0 untuk posisi yang di-mask\n",
    "\n",
    "        # Normalisasi\n",
    "        attn_weights = self.softmax(dp_attention)\n",
    "        # attn_weights: [batch_size, seq_len, seq_len]\n",
    "\n",
    "        # Hitung output\n",
    "        output = torch.matmul(attn_weights, V)\n",
    "        # output: [batch_size, seq_len, d_model]\n",
    "\n",
    "        return output, attn_weights\n",
    "        \n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # x: [batch_size, seq_len, d_model]\n",
    "\n",
    "        # Hitung Q, K, V\n",
    "        Q = self.q_linear(x)\n",
    "        K = self.k_linear(x)\n",
    "        V = self.v_linear(x)\n",
    "        # Q, K, V: [batch_size, seq_len, d_model]\n",
    "\n",
    "        # Scaled Dot product attention\n",
    "        attn_output, attn_weights = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        output = self.out_linear(attn_output)\n",
    "        # output: [batch_size, seq_len, d_model]\n",
    "        \n",
    "        return output, attn_weights\n",
    "    \n",
    "# Contoh penggunaan SelfAttention\n",
    "batch_size = 2\n",
    "seq_len = 5\n",
    "\n",
    "self_attention_layer = SelfAttention(d_model)\n",
    "x_self_attention = pe_output.view(batch_size, seq_len, d_model)  # [batch_size, seq_len, d_model]\n",
    "attn_output, attn_weights = self_attention_layer(x_self_attention)\n",
    "\n",
    "print(\"Shape output SelfAttention:\", attn_output.shape)  # [batch_size, seq_len, d_model]\n",
    "print(\"Shape attention weights:\", attn_weights.shape)  # [batch_size, seq_len, seq_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape output MultiheadSelfAttention: torch.Size([2, 5, 512])\n",
      "Shape attention weights MultiheadSelfAttention: torch.Size([2, 8, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "class MultiheadSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads=8):\n",
    "        super(MultiheadSelfAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        assert d_model % n_heads == 0, \"d_model harus dibagi habis oleh n_heads\"\n",
    "\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        # self.dropout = nn.Dropout(0.1) # Optional\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        # Q, K, V: [batch_size, n_heads, seq_len, d_k]\n",
    "\n",
    "        # Hitung dot product attention\n",
    "        dp_attention = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        # dp_attention: [batch_size, n_heads, seq_len, seq_len]\n",
    "    \n",
    "        if mask is not None:\n",
    "             # Mask: [batch_size, 1, seq_len, seq_len] or [batch_size, n_heads, seq_len, seq_len] (broadcastable)\n",
    "            dp_attention = dp_attention.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        # Normalisasi\n",
    "        attn_weights = self.softmax(dp_attention)\n",
    "        # attn_weights: [batch_size, n_heads, seq_len, seq_len]\n",
    "\n",
    "        # Hitung output\n",
    "        output = torch.matmul(attn_weights, V)\n",
    "        # output: [batch_size, n_heads, seq_len, d_k]\n",
    "\n",
    "        return output, attn_weights\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        # x: [batch_size, seq_len, d_model]\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        x = x.view(batch_size, seq_length, self.n_heads, self.d_k).permute(0, 2, 1, 3)\n",
    "        # x: [batch_size, n_heads, seq_len, d_k]\n",
    "        return x\n",
    "    \n",
    "    def concat_heads(self, x):\n",
    "        # x: [batch_size, n_heads, seq_len, d_k]\n",
    "        batch_size, n_heads, seq_length, d_k = x.size()\n",
    "        x = x.permute(0, 2, 1, 3).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        # x: [batch_size, seq_len, d_model]\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        # x: [batch_size, seq_len, d_model]\n",
    "        Q = self.split_heads(self.q_linear(q))\n",
    "        K = self.split_heads(self.k_linear(k))\n",
    "        V = self.split_heads(self.v_linear(v))\n",
    "        \n",
    "        attn_output, attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        output = self.concat_heads(attn_output)\n",
    "        output = self.out_proj(output)\n",
    "        # output: [batch_size, seq_len, d_model]\n",
    "\n",
    "        return output, attention_weights\n",
    "    \n",
    "# Contoh penggunaan MultiheadSelfAttention\n",
    "batch_size = 2\n",
    "seq_len = 5\n",
    "\n",
    "multihead_attention_layer = MultiheadSelfAttention(d_model, n_heads=8)\n",
    "x_multihead_attention = pe_output.view(batch_size, seq_len, d_model)  # [batch_size, seq_len, d_model]\n",
    "attn_output_multihead, attn_weights_multihead = multihead_attention_layer(x_multihead_attention, x_multihead_attention, x_multihead_attention) # [batch_size, seq_len, d_model]\n",
    "\n",
    "print(\"Shape output MultiheadSelfAttention:\", attn_output_multihead.shape)  # [batch_size, seq_len, d_model]\n",
    "print(\"Shape attention weights MultiheadSelfAttention:\", attn_weights_multihead.shape)  # [batch_size, n_heads, seq_len, seq_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(FeedForward, self).__init__()\n",
    "        # Biasanya d_ff lebih besar dari d_model\n",
    "         # d_model = 512\n",
    "        # d_ff = 4 * d_model\n",
    "       \n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.relu = nn.ReLU() # bisa diganti dengan fungsi aktivasi lain tapi yg linear, misalnya GELU\n",
    "        # self.dropout = nn.Dropout(0.1) # Opsional\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, d_model]\n",
    "        output = self.linear1(x)\n",
    "        output = self.relu(output)\n",
    "        # output = self.dropout(output)\n",
    "        output = self.linear2(output)\n",
    "\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape output EncoderLayer: torch.Size([2, 5, 512])\n",
      "Shape attention weights EncoderLayer: torch.Size([2, 8, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.attention = MultiheadSelfAttention(d_model, n_heads)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.ffn = FeedForward(d_model, d_ff)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # x: [batch_size, seq_len, d_model]\n",
    "\n",
    "        # Multi-head self-attention\n",
    "        attn_output, attn_weights = self.attention(x, x, x, mask=mask)  # -> [batch, seq_len, d_model]\n",
    "\n",
    "        # Residual connection + LayerNorm\n",
    "        x = self.norm1(x + self.dropout1(attn_output))\n",
    "\n",
    "        # Feedforward network\n",
    "        ffn_output = self.ffn(x)  # -> [batch, seq_len, d_model]\n",
    "\n",
    "        # Residual connection + LayerNorm\n",
    "        x = self.norm2(x + self.dropout2(ffn_output))\n",
    "\n",
    "        return x, attn_weights\n",
    "    \n",
    "# Contoh penggunaan\n",
    "d_model = 512\n",
    "n_heads = 8\n",
    "d_ff = 2048  # Biasanya d_ff lebih besar dari d_model\n",
    "transformer_encoder = EncoderLayer(d_model, n_heads, d_ff)\n",
    "\n",
    "# Input untuk TransformerEncoder\n",
    "x_transformer_encoder = pe_output.view(batch_size, seq_len, d_model)  # [batch_size, seq_len, d_model]\n",
    "encoder_output, encoder_attn_weights = transformer_encoder(x_transformer_encoder)\n",
    "print(\"Shape output EncoderLayer:\", encoder_output.shape)  # [batch_size, seq_len, d_model]\n",
    "print(\"Shape attention weights EncoderLayer:\", encoder_attn_weights.shape)  # [batch_size, n_heads, seq_len, seq_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5000, 512])\n",
      "torch.Size([5000, 1])\n",
      "torch.Size([2, 5, 512])\n",
      "Shape output TransformerEncoder: torch.Size([2, 5, 512])\n",
      "Output TransformerEncoder: tensor([[[-0.0671,  0.0982,  0.6902,  ..., -0.6005,  1.9808,  1.4554],\n",
      "         [ 0.5984,  0.3918,  1.1752,  ...,  0.0959, -1.3847, -1.4753],\n",
      "         [ 0.1981,  1.3724, -0.0871,  ...,  0.8550, -0.1228,  0.0693],\n",
      "         [ 0.8066,  2.1204,  0.5496,  ...,  0.8747,  0.4876,  0.2374],\n",
      "         [ 0.9034,  2.2935, -1.6426,  ...,  0.6958,  0.5359, -1.5127]],\n",
      "\n",
      "        [[-2.7358,  1.7557, -1.9190,  ...,  1.3079, -0.4140,  0.3887],\n",
      "         [-0.6214,  0.7021, -0.4069,  ...,  1.3759, -1.4253, -0.1855],\n",
      "         [-0.8720, -0.6711, -0.9085,  ...,  0.0365, -0.2451,  0.6557],\n",
      "         [-0.7131,  0.4458, -0.7062,  ...,  1.2501, -0.9426, -0.9888],\n",
      "         [-0.2770,  1.6805, -0.5702,  ...,  0.8715, -1.9425,  0.3065]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransformerEncoder(\n",
       "  (embedding): WordEmbedding(\n",
       "    (embedding): Embedding(10000, 512, padding_idx=0)\n",
       "  )\n",
       "  (positional_encoding): PositionalEncoding()\n",
       "  (layers): ModuleList(\n",
       "    (0-5): 6 x EncoderLayer(\n",
       "      (attention): MultiheadSelfAttention(\n",
       "        (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (softmax): Softmax(dim=-1)\n",
       "      )\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (ffn): FeedForward(\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_heads, d_ff, num_layers, max_len=5000, dropout=0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.embedding = WordEmbedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_len)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # x: [batch_size, seq_len]\n",
    "        x = self.embedding(x)  # -> [batch_size, seq_len, d_model]\n",
    "        x = self.positional_encoding(x)  # -> [batch_size, seq_len, d_model]\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x, _ = layer(x, mask=mask)\n",
    "\n",
    "        x = self.norm(x)  # Normalisasi akhir\n",
    "        return x\n",
    "    \n",
    "# Contoh penggunaan TransformerEncoder\n",
    "vocab_size = 10000\n",
    "d_model = 512\n",
    "n_heads = 8\n",
    "d_ff = 2048\n",
    "num_layers = 6  # Jumlah layer encoder\n",
    "transformer_encoder = TransformerEncoder(vocab_size, d_model, n_heads, d_ff, num_layers)\n",
    "\n",
    "# Input untuk TransformerEncoder\n",
    "x_transformer_encoder = token_ids  # [batch_size, seq_len]\n",
    "encoder_output = transformer_encoder(x_transformer_encoder)\n",
    "print(\"Shape output TransformerEncoder:\", encoder_output.shape)  # [batch_size, seq_len, d_model]\n",
    "print(\"Output TransformerEncoder:\", encoder_output)  # Output dari TransformerEncoder\n",
    "transformer_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape output DecoderLayer: torch.Size([2, 5, 512])\n",
      "Output DecoderLayer: tensor([[[ 1.3042,  1.0292, -1.4429,  ...,  1.0451,  1.0885, -1.5961],\n",
      "         [-0.0095,  0.4970, -0.1694,  ...,  0.9962,  1.0766,  0.1079],\n",
      "         [-1.4791, -0.9519,  0.9839,  ...,  0.8316,  0.4817, -1.8652],\n",
      "         [-0.8098, -0.9606, -1.0625,  ...,  2.1481, -0.5122, -0.1365],\n",
      "         [-0.6694, -1.0202,  1.3164,  ..., -0.9272,  2.2489,  0.4197]],\n",
      "\n",
      "        [[ 0.4735, -0.1348, -0.4349,  ..., -0.1836,  0.9704, -0.5184],\n",
      "         [-0.9053, -1.5357, -0.1046,  ...,  1.5170,  0.1962, -0.8287],\n",
      "         [-0.1218,  1.5366, -2.0503,  ..., -0.9942,  2.2141, -0.2618],\n",
      "         [ 0.6441, -0.8123, -0.7478,  ..., -1.2165,  0.3667,  1.2559],\n",
      "         [ 0.4269, -1.3934, -0.2759,  ..., -0.0729, -0.5538, -0.2161]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.self_attn = MultiheadSelfAttention(d_model, n_heads)\n",
    "        self.cross_attn = MultiheadSelfAttention(d_model, n_heads)\n",
    "        self.ffn = FeedForward(d_model, d_ff)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, y, tgt_mask=None, cross_att_mask=None):\n",
    "        # x: [batch_size, tgt_seq_len, d_model]\n",
    "        # y/encoder_ouput: [batch_size, src_seq_len, d_model]\n",
    "\n",
    "        # Masked self-attention (self attention punya decoder)\n",
    "        self_attn_output, _ = self.self_attn(q=x, k=x, v=x, mask=tgt_mask)\n",
    "\n",
    "        # Residual connection + LayerNorm (Self-Attention Block)\n",
    "        # Residual ditambahkan ke input self-attention (yaitu tgt)\n",
    "        x = self.norm1(x + self.dropout1(self_attn_output))\n",
    "\n",
    "        # Cross-attention (attention antara decoder dan encoder)\n",
    "        cross_attn_output, _ = self.cross_attn(q=x,  # query = decoder Q\n",
    "                                               k=y,  # key = encoder K\n",
    "                                               v=y,    # value = encoder V\n",
    "                                               mask=cross_att_mask)  \n",
    "        \n",
    "        # Residual connection + LayerNorm (Cross-Attention Block)\n",
    "        # Residual ditambahkan ke input cross-attention (yaitu tgt setelah norm1)\n",
    "        x = self.norm2(x + self.dropout2(cross_attn_output)) # tgt di sisi kanan adalah input ke cross-attn\n",
    "\n",
    "        # Feed Forward Network\n",
    "        ffn_output = self.ffn(x)\n",
    "\n",
    "        # Residual connection + LayerNorm (FFN Block)\n",
    "        # Residual ditambahkan ke input FFN (yaitu tgt setelah norm2)\n",
    "        x = self.norm3(x + self.dropout3(ffn_output)) # tgt di sisi kanan adalah input ke FFN\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# Contoh penggunaan DecoderLayer\n",
    "d_model = 512\n",
    "n_heads = 8\n",
    "d_ff = 2048  # Biasanya d_ff lebih besar dari d_model\n",
    "batch_size = 2\n",
    "seq_len = 5\n",
    "decoder_layer = DecoderLayer(d_model, n_heads, d_ff)\n",
    "\n",
    "# Input untuk DecoderLayer\n",
    "x_decoder = pe_output.view(batch_size, seq_len, d_model)  # [batch_size, seq_len, d_model]\n",
    "encoder_output = encoder_output  # Output dari TransformerEncoder # [batch_size, seq_len, d_model]\n",
    "decoder_output = decoder_layer(x_decoder, encoder_output)\n",
    "print(\"Shape output DecoderLayer:\", decoder_output.shape)  # [batch_size, seq_len, d_model]\n",
    "print(\"Output DecoderLayer:\", decoder_output)  # Output dari DecoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5000, 512])\n",
      "torch.Size([5000, 1])\n",
      "Shape output TransformerDecoder: torch.Size([2, 5, 512])\n",
      "Output TransformerDecoder: tensor([[[-1.6022, -0.5740, -0.9025,  ...,  0.9632, -0.9350, -0.3362],\n",
      "         [-2.5412, -0.8257, -0.1859,  ...,  0.9046, -0.7554,  0.0827],\n",
      "         [-1.9336, -0.4975, -0.1599,  ...,  0.5034,  0.0300, -0.3450],\n",
      "         [-1.9409, -0.7423, -0.3439,  ...,  0.3450,  0.1998, -0.5473],\n",
      "         [-0.6115, -2.0115, -0.9331,  ...,  1.2873,  0.0926,  0.2538]],\n",
      "\n",
      "        [[ 0.1636, -1.0241,  0.1872,  ...,  1.7562, -2.5314, -0.4062],\n",
      "         [-1.6630, -0.3286,  1.5659,  ...,  0.9768, -0.5189, -0.7102],\n",
      "         [ 0.1092, -0.7213,  0.6141,  ...,  1.4317, -1.4953,  0.8493],\n",
      "         [-0.8203, -0.2893,  0.6903,  ...,  1.1725, -1.5588, -0.2430],\n",
      "         [-0.0904,  0.5841,  0.9267,  ...,  0.1071, -1.9865, -1.5323]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_heads, d_ff, num_layers, max_len=5000, dropout=0.1):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.embedding = WordEmbedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_len)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, n_heads, d_ff) for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, encoder_output, tgt_mask=None, cross_att_mask=None):\n",
    "        # x: [batch_size, tgt_seq_len]\n",
    "        x = self.embedding(x)  # -> [batch_size, tgt_seq_len, d_model]\n",
    "        x = self.positional_encoding(x)  # -> [batch_size, tgt_seq_len, d_model]\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, tgt_mask=tgt_mask, cross_att_mask=cross_att_mask)\n",
    "\n",
    "        x = self.norm(x)  # Normalisasi akhir\n",
    "        return x\n",
    "    \n",
    "# Contoh penggunaan TransformerDecoder\n",
    "vocab_size = 10000\n",
    "d_model = 512\n",
    "n_heads = 8\n",
    "d_ff = 2048\n",
    "num_layers = 6  # Jumlah layer decoder\n",
    "transformer_decoder = TransformerDecoder(vocab_size, d_model, n_heads, d_ff, num_layers)\n",
    "\n",
    "# Input untuk TransformerDecoder\n",
    "x_transformer_decoder = token_ids  # [batch_size, tgt_seq_len]\n",
    "decoder_output = transformer_decoder(x_transformer_decoder, encoder_output)\n",
    "print(\"Shape output TransformerDecoder:\", decoder_output.shape)  # [batch_size, tgt_seq_len, d_model]\n",
    "print(\"Output TransformerDecoder:\", decoder_output)  # Output dari TransformerDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5000, 512])\n",
      "torch.Size([5000, 1])\n",
      "torch.Size([5000, 512])\n",
      "torch.Size([5000, 1])\n",
      "torch.Size([2, 5, 512])\n",
      "Shape of final Transformer output: torch.Size([2, 5, 10000])\n"
     ]
    }
   ],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_heads, d_ff, num_layers, max_len=5000, dropout=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = TransformerEncoder(vocab_size, d_model, n_heads, d_ff, num_layers, max_len, dropout)\n",
    "        self.decoder = TransformerDecoder(vocab_size, d_model, n_heads, d_ff, num_layers,  max_len, dropout)\n",
    "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "        \"\"\"\n",
    "        src: Source sequence token IDs [batch_size, src_seq_len]\n",
    "        tgt: Target sequence token IDs [batch_size, tgt_seq_len]\n",
    "        \"\"\"\n",
    "        \n",
    "        # Encoder processes the source sequence\n",
    "        encoder_output = self.encoder(src, mask=src_mask)\n",
    "\n",
    "        # Decoder processes the target sequence and the encoder's output\n",
    "        # Note: The cross-attention mask is typically the source mask (src_mask)\n",
    "        decoder_output = self.decoder(tgt, encoder_output, tgt_mask=tgt_mask, cross_att_mask=src_mask)\n",
    "\n",
    "        # Final linear layer to get vocabulary scores\n",
    "        output = self.output_layer(decoder_output)\n",
    "\n",
    "        return output\n",
    "    \n",
    "vocab_size = 10000\n",
    "d_model = 512\n",
    "n_heads = 8\n",
    "d_ff = 2048\n",
    "num_layers = 6\n",
    "\n",
    "transformer_model = Transformer(vocab_size, d_model, n_heads, d_ff, num_layers)\n",
    "\n",
    "# Example source and target tensors\n",
    "src_token_ids = torch.tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]) # (batch_size=2, seq_len=5)\n",
    "tgt_token_ids = torch.tensor([[11, 12, 13, 0, 0], [14, 15, 16, 17, 0]]) # (batch_size=2, seq_len=5)\n",
    "\n",
    "\n",
    "# Input for the corrected Transformer\n",
    "transformer_output = transformer_model(src=src_token_ids, tgt=tgt_token_ids)\n",
    "print(\"Shape of final Transformer output:\", transformer_output.shape) # Should be [batch_size, seq_len, vocab_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
