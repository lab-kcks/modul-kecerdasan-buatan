{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Import Library**\n",
    "\n",
    "Sebelum membuat Transformer from scratch, import library terlebih dahulu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/soic/labby/modul-kecerdasan-buatan/.venv/lib/python3.12/site-packages/torch/_subclasses/functional_tensor.py:275: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Positional Encoding**\n",
    "\n",
    "Karena Transformer tidak mengandalkan urutan alami dari data (seperti urutan waktu pada RNN), informasi posisi token ditambahkan secara eksplisit melalui positional encoding. Vektor ini menggunakan fungsi sinus dan cosinus untuk memberikan “koordinat” yang unik pada tiap token sehingga model dapat memahami urutan kata dalam kalimat. \n",
    "\n",
    "Setiap token dalam input ditambahkan dengan vektor posisi yang dihitung menggunakan fungsi sinus dan cosinus. Vektor ini memiliki dimensi yang sama dengan embedding sehingga penjumlahan antara embedding dan positional encoding dapat dilakukan secara langsung.\n",
    "\n",
    "Positional encoding memberikan representasi unik berbasis pola periodik, mirip dengan bagaimana gelombang sinusoidal dapat merepresentasikan informasi dalam sinyal, sehinnga bisa dianggap seperti memberikan `koordinat` untuk setiap token. Kenapa fungsi sinus dan cosinus? Karena Penggunaan fungsi sinus dan cosinus memastikan bahwa posisi relatif antara kata-kata tetap terjaga, bahkan ketika panjang urutan bervariasi. Hal ini penting karena Transformer tidak memiliki mekanisme memori seperti RNN.\n",
    "\n",
    "Rumus dari Postional Encoding adalah sebagai berikut:\n",
    "$$\n",
    "PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{\\frac{2i}{d}}}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{\\frac{2i}{d}}}\\right)\n",
    "$$\n",
    "\n",
    "- `d` adalah dimensi embedding\n",
    "- `pos` adalah index dari posisi\n",
    "- `i` adalah index dari dimensi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # Buat tensor posisi dengan ukuran (max_len, d_model)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        print(pe.shape)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        print(position.shape)\n",
    "        # Hitung pembagi frekuensi\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        # Aplikasi rumus sinus dan cosinus\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        # Tambahkan dimensi batch agar dapat langsung ditambahkan ke embedding\n",
    "        pe = pe.unsqueeze(0)  # Shape: [1, max_len, d_model]\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, d_model]\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5000, 512])\n",
      "torch.Size([5000, 1])\n",
      "Shape input setelah positional encoding: torch.Size([1, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "# Contoh penggunaan:\n",
    "d_model = 512\n",
    "pe_layer = PositionalEncoding(d_model)\n",
    "x = torch.zeros(1, 10, d_model)  # Misal sequence length 10\n",
    "x_pos = pe_layer(x)\n",
    "print(\"Shape input setelah positional encoding:\", x_pos.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
